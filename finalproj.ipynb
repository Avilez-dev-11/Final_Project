{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7066723,"sourceType":"datasetVersion","datasetId":4069149},{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Published on December 12, 2023. By Marília Prata, mpwolke ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-24T03:43:30.451677Z","iopub.execute_input":"2024-04-24T03:43:30.451966Z","iopub.status.idle":"2024-04-24T03:43:30.473964Z","shell.execute_reply.started":"2024-04-24T03:43:30.451940Z","shell.execute_reply":"2024-04-24T03:43:30.472999Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/en-fr-translation-dataset/en-fr.csv\n/kaggle/input/mental-health-chatbot-pairs/train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Citation: \n\nBigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022\n\n\" BLOOM is able to generate text in 46 natural languages and 13 programming languages. For almost all of them, such as Spanish, French and Arabic, BLOOM will be the first language model with over 100B parameters ever created. This is the culmination of a year of work involving over 1000 researchers from 70+ countries and 250+ institutions, leading to a final run of 117 days training the BLOOM model on the Jean Zay supercomputer in the south of Paris, France thanks to a compute grant worth an estimated €3M from French research agencies CNRS and GENCI.\"\n\nhttps://bigscience.huggingface.co/blog/bloom","metadata":{}},{"cell_type":"markdown","source":"#We have Human and Assistant","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/en-fr-translation-dataset/en-fr.csv')\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:43:30.475244Z","iopub.execute_input":"2024-04-24T03:43:30.475514Z","iopub.status.idle":"2024-04-24T03:47:33.143847Z","shell.execute_reply.started":"2024-04-24T03:43:30.475490Z","shell.execute_reply":"2024-04-24T03:47:33.142625Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                         en  \\\n22520371  Only with a highly overcompensatory stock–recr...   \n22520372  The model predicts that the assumption made ab...   \n22520373  Overall the results confirm the unsatisfactory...   \n22520374  Error 404 — file not found Sorry, but the file...   \n22520375  British Columbia Lodging and Campgrounds Assoc...   \n\n                                                         fr  \n22520371  C'est seulement en cas de courbe stock–recrute...  \n22520372  Le modèle prévoit que l'hypothèse émise au suj...  \n22520373  Dans l'ensemble, les résultats confirment le p...  \n22520374  Erreur 404 — fichier introuvable Nous sommes d...  \n22520375  British Columbia Lodging and Campgrounds Assoc...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22520371</th>\n      <td>Only with a highly overcompensatory stock–recr...</td>\n      <td>C'est seulement en cas de courbe stock–recrute...</td>\n    </tr>\n    <tr>\n      <th>22520372</th>\n      <td>The model predicts that the assumption made ab...</td>\n      <td>Le modèle prévoit que l'hypothèse émise au suj...</td>\n    </tr>\n    <tr>\n      <th>22520373</th>\n      <td>Overall the results confirm the unsatisfactory...</td>\n      <td>Dans l'ensemble, les résultats confirment le p...</td>\n    </tr>\n    <tr>\n      <th>22520374</th>\n      <td>Error 404 — file not found Sorry, but the file...</td>\n      <td>Erreur 404 — fichier introuvable Nous sommes d...</td>\n    </tr>\n    <tr>\n      <th>22520375</th>\n      <td>British Columbia Lodging and Campgrounds Assoc...</td>\n      <td>British Columbia Lodging and Campgrounds Assoc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\n\nAuthors: Probably, that's the longest Authors list I've ever seen. There are 3 pages with their names on the pdf document.\n\nBigScience Workshop: Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas et al. (293 additional authors not shown)\n\n\"Pretrained language models have become a cornerstone of modern natural language processing (NLP) pipelines because they often produce better performance from smaller quantities of labeled data. The development of ELMo, ULMFiT (Howard and Ruder, 2018), GPT (Radford et al., 2018), and BERT led to the widespread use of pretrained models as an initialization for finetuning on downstream tasks.\"\n\n\"The subsequent finding that pretrained language models can perform useful tasks without\nany additional training) further demonstrated their utility. In addition, the empirical observation that a language model’s performance tends to increase as the model is made larger—sometimes predictably and sometimes suddenly has led to a  trend of increasing scale. Apart from environmental concerns, the costs of training large language models (LLMs) are only affordable for well-resourced organizations.\"\n\n\"As a result, the majority of the research community has been excluded from the development of LLMs. This exclusion has had concrete consequences; for example, most LLMs are primarily trained on English-language text (with notable exceptions in Chinese and Korean).\n\n\"BLOOM is a 176 billion parameter language model trained on 46 natural languages and 13 programming languages that was developed and released by a collaboration of hundreds of researchers. The compute for training BLOOM was provided through a French public grant from GENCI and IDRIS, leveraging IDRIS’ Jean Zay supercomputer. To build BLOOM, the authors undertook a thorough design process for each of its components, including the training dataset, model architecture and training objective, and engineering strategy for distributed learning. The authors also performed an analysis of the model’s capabilities. Their overall aim is not only to publicly release a large-scale multilingual language model with performance comparable to recently developed systems, but also to document the coordinated process that went into its development. The purpose of this paper is to provide a high-level overview of these design steps while referencing the individual reports they produced over the course of developing BLOOM.\"\n\nhttps://arxiv.org/abs/2211.05100","metadata":{}},{"cell_type":"code","source":"#4th row, 1st column   \n\ndf.iloc[44,0]","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:53:31.170905Z","iopub.execute_input":"2024-04-24T03:53:31.171897Z","iopub.status.idle":"2024-04-24T03:53:31.178458Z","shell.execute_reply.started":"2024-04-24T03:53:31.171848Z","shell.execute_reply":"2024-04-24T03:53:31.177189Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Instruments The ancient Egyptians use the transit for the first time about 4,000 years ago.'"},"metadata":{}}]},{"cell_type":"markdown","source":"#Social Limitations of LLM Development\n\n\"While the continued increase in the size of large language models has resulted in improvements across a wide range of tasks, it has also exacerbated issues with their development and use. The computational expense of large models also prohibits the majority of the research community from participating in their development, evaluation and routine use. Moreover, the computational costs have also lead to concerns about the carbon footprint stemming from the training and use of large language models, and existing carbon footprint studies have likely under-estimated emissions. Contributing to an increase in the global carbon footprint exacerbates climate change which most severely affects already-marginalized communities. Furthermore, the concentration of resources within a handful of (typically industrial) institutions with primarily technical expertise hinders prospects for an inclusive, collaborative, and reliable governance of the technology.\"\n\n\"Despite the substantial social dangers in allowing this technology to be developed unilaterally by corporations, EleutherAI was the only non-corporate entity outside of China that was developing large language models before the BigScience Workshop was convened.\"\n\nhttps://arxiv.org/pdf/2211.05100.pdf","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:53:40.474047Z","iopub.execute_input":"2024-04-24T03:53:40.474376Z","iopub.status.idle":"2024-04-24T03:53:40.478803Z","shell.execute_reply.started":"2024-04-24T03:53:40.474352Z","shell.execute_reply":"2024-04-24T03:53:40.477881Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#Ethical Considerations within BigScience \n\n\"In order to acknowledge and start addressing social limitations of LLM development within BigScience, the workshop relied on a collaboratively designed Ethical Charter and original research on applicable regulations in jurisdictions outside of the US to guide its choices throughout the project. In particular, the charter emphasizes values of inclusivity and diversity, openness and reproducibility, and responsibility in various aspects of the organization. Each of these values are showcased in different ways in the dataset curation, modeling, engineering, evaluation, and other social impact (throughout) aspects of the project.\"\n\n\"BLOOM was trained on the ROOTS corpus, a composite collection of 498 Hugging Face datasets amounting to 1.61 terabytes of text that span 46 natural languages and 13 programming languages.\"\n\nMOTIVATION - Under-valued \"Data Work\"-“High-quality” data for as little cost \n\n\"The disconnect between developers and (in)voluntary users of the technology is particularly apparent in the curation of the datasets that have supported recent large-scale machine learning projects, where intentional “Data work” is generally under-valued.\"\n\n\"In the context of LLMs, this tendency is exemplified by a range of heuristics-based filtering approaches that prioritize getting as much “high-quality” data for as little cost as possible over engaging with the needs—and rights—of data subjects, where quality is commonly defined as maximizing performance on downstream tasks while occasionally removing content deemed offensive by the developers.\"\n\nLANGUAGE CHOICES\n\n\"These considerations led us to an incremental process for choosing which languages were to be included in the corpus. The authors started with a list of eight of the world’s largest languages by number of speakers for which they did active outreach in the\nearly stages of the project to invite fluent speakers to join the data efforts. Then, on the recommendation of language communities they expanded Swahili in the original selection to the category of Niger-Congo languages, and Hindi and Urdu to Indic languages.\n\n\"Finally, the authors proposed that any group of 3 or more participants fluent in an additional language could add it to the supported list if they would commit to selecting sources and guiding processing choices in the language in order to avoid common issues with corpora selected through automatic language identification without specific language expertise.\"\n\n\"QUALITY\" FILTERING: TEXT PRODUCED by Humans For HUMANS \n\n\"After obtaining the text, we found that most of the sources contained some amount of text that was not naturallanguage, for example preprocessing errors, SEO pages, or spam (including pornographic spam).\"\n\n\" In order to filter non-natural language, the authors defined a set of quality indicators, where high-quality text is defined as “written by humans for humans”, without distinction of content (as they wanted content selection to exclusively be the domain of the more accountable human source selection) or a priori judgments of grammaticality.\"\n\nhttps://arxiv.org/pdf/2211.05100.pdf","metadata":{}},{"cell_type":"code","source":"# Set up the GPU device\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:53:45.653476Z","iopub.execute_input":"2024-04-24T03:53:45.653839Z","iopub.status.idle":"2024-04-24T03:53:45.658248Z","shell.execute_reply.started":"2024-04-24T03:53:45.653810Z","shell.execute_reply":"2024-04-24T03:53:45.657364Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#Downloading the model bigscience/bloom-1b7","metadata":{}},{"cell_type":"code","source":"#By Man of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\nfrom transformers import BloomTokenizerFast, BloomForCausalLM\n\ntokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b7\")\nmodel = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:53:47.334456Z","iopub.execute_input":"2024-04-24T03:53:47.335093Z","iopub.status.idle":"2024-04-24T03:54:11.708205Z","shell.execute_reply.started":"2024-04-24T03:53:47.335059Z","shell.execute_reply":"2024-04-24T03:54:11.707179Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdf7353283db45b18f6e402b9a4a8aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c557314ad3471280aa7c036b9e92b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f64ac31235b1411398abcb7ec271d950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8e6a58299204303b341fa23a302374c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50a7b2b2b7646f28db110ae19052f7f"}},"metadata":{}}]},{"cell_type":"markdown","source":"#PROMPTS\n \n\"Based on recent research on the impact of prompting on language model performance, the authors decided to build a language model evaluation suite that allowed them to vary both the basic task data as well as the prompting that is used to contextualize the task. Their prompts were developed prior to BLOOM’s release, and did not undergo any a priori refinement using models. That is, the prompts they used in their evaluation are ones that humans believed were a reasonable way to solicit the desired task behavior from a language model. Their goal for designing prompts in this way is to simulate realistic zero-shot or one-shot results that a new user could expect from BLOOM. This is in contrast to presenting best-case performances that might result from multiple rounds of trial-and-error on prompt design.\"\n\n\"The authors chose to report the former because the latter is harder to reproduce systematically, is arguably a less representative picture of how the model works in the average setting, and is not representative of true zero-shot learning where no labeled data is available. They generated multiple prompts per task using promptsource.\"\n\n\"They followed the procedure, in which prompt generation is crowd-sourced, and thus they saw substantial variety in length and style across prompts. To improve quality and clarity, multiple peer reviews were performed on each prompt for artifacts and consistency.\"\n\n\"They also generated prompts for many tasks that are not included in this paper due to resource constraints. All of their prompts for all tasks (both those analyzed in the paper and those not yet analyzed) are publicly available.\"\n\nOn Baseline models:  T0, a variant of T5 that underwent multitask prompted finetuning on datasets from P3\n\n#WMT - Best Prompts, the more Verbose ones\n\n\"The best prompts tend to be the more verbose ones; the “version-target” prompt is consistently better and the “gpt3-target” and “xglm-source+target” prompts have very poor performance, especially for zero-shot.\"\n\n\"In the one-shot setting, BLOOM can, with the right prompt, perform competent translation, although it is behind dedicated (supervised) models such as M2M-100 (43.8 BLEU for English→French and 40.4 for French→English, compared to 34.2 and 35.4 BLEU for BLOOM). The two major problems observed, particularly in the zero-shot setting, are (i) over-generation and (ii) not producing the correct language (an obvious prerequisite for a good translation). Both of these aspects are greatly improved as the number of few-shot examples is increased.\"\n\nhttps://arxiv.org/pdf/2211.05100.pdf\n","metadata":{}},{"cell_type":"markdown","source":"#Turn ON GPU!","metadata":{}},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:54:11.710840Z","iopub.execute_input":"2024-04-24T03:54:11.711263Z","iopub.status.idle":"2024-04-24T03:54:13.681453Z","shell.execute_reply.started":"2024-04-24T03:54:11.711222Z","shell.execute_reply":"2024-04-24T03:54:13.680223Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#Multilingual Probing\n\n\"Probing has emerged as a significant evaluation paradigm to analyze and interpret the inner workings of LLMs although it comes with certain shortcomings. Examination of the LLM embeddings can help shed light on the generalizing abilities of the model apart from its training objective loss or downstream task evaluation, which is especially beneficial for examining languages lacking annotated datasets or benchmarks.\"\n\nCorrelation\n\n\"The authors run statistical tests to analyze correlations between the probing performance and linguistic, dataset, and model configuration criteria:\"\n\nProbing and pretraining dataset size: the authors run the Pearson correlation coefficient test (Pearson, 1895) to compute correlation between the probing performance and these\ndata configuration criteria.\"\n\n\"Effect of the model size: the results are divided into two groups by the BLOOM\nversion. Here, they used the Mann-Whitney U test to see if there is a correlation between\nthe number of parameters and the probing results.\"\n\n#Probing Results\n\n\"The results of probing experiments averaged over the probing tasks and experiment runs within each language. The overall pattern is that BLOOM-1B7 performs on par or better than BLOOM, and both LLMs outperform the count-based baselines. In particular, the LLMs achieve more robust performance on Arabic, Basque, and Indo-European languages (e.g., Catalan, French, Hindi, Portuguese, Spanish, and Urdu), while Bengali, Wolof, and Yoruba receive the lowest scores. The authors attributed this behavior to the transfer abilities: BLOOM infers linguistic properties better for the closely related languages that comprise a significant amount of data.\" \n\nMultilingual Abilities.\n\n\"A separate research interest implies considering languages that are not explicitly included in the pretraining corpus of the models. Expanding the set of languages for probing will allow for a typological interpretation and a deeper analysis of the most learnable and hard-to-learn linguistic features on a more considerable scope.\"\n\nhttps://arxiv.org/pdf/2211.05100.pdf","metadata":{}},{"cell_type":"code","source":"text = df.loc[44,'fr']","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:57:17.216299Z","iopub.execute_input":"2024-04-24T03:57:17.217007Z","iopub.status.idle":"2024-04-24T03:57:17.222516Z","shell.execute_reply.started":"2024-04-24T03:57:17.216963Z","shell.execute_reply":"2024-04-24T03:57:17.220903Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"en_text = df.loc[44,'en']","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:58:16.202265Z","iopub.execute_input":"2024-04-24T03:58:16.202991Z","iopub.status.idle":"2024-04-24T03:58:16.207743Z","shell.execute_reply.started":"2024-04-24T03:58:16.202955Z","shell.execute_reply":"2024-04-24T03:58:16.206498Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:57:19.357384Z","iopub.execute_input":"2024-04-24T03:57:19.358067Z","iopub.status.idle":"2024-04-24T03:57:19.364069Z","shell.execute_reply.started":"2024-04-24T03:57:19.358030Z","shell.execute_reply":"2024-04-24T03:57:19.362997Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'Instruments Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ.'"},"metadata":{}}]},{"cell_type":"code","source":"en_text","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:58:35.174226Z","iopub.execute_input":"2024-04-24T03:58:35.174601Z","iopub.status.idle":"2024-04-24T03:58:35.181159Z","shell.execute_reply.started":"2024-04-24T03:58:35.174572Z","shell.execute_reply":"2024-04-24T03:58:35.180218Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'Instruments The ancient Egyptians use the transit for the first time about 4,000 years ago.'"},"metadata":{}}]},{"cell_type":"code","source":"#By Man of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\ninputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=128, num_return_sequences=4, num_beams=4)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:57:31.139465Z","iopub.execute_input":"2024-04-24T03:57:31.139825Z","iopub.status.idle":"2024-04-24T03:57:37.040413Z","shell.execute_reply.started":"2024-04-24T03:57:31.139796Z","shell.execute_reply":"2024-04-24T03:57:37.039541Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:57:40.254981Z","iopub.execute_input":"2024-04-24T03:57:40.255323Z","iopub.status.idle":"2024-04-24T03:57:40.262718Z","shell.execute_reply.started":"2024-04-24T03:57:40.255282Z","shell.execute_reply":"2024-04-24T03:57:40.261664Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Instruments Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000 ans avant Jésus-Christ. Les Égyptiens utilisent le transit pour la première fois, environ 2000\n","output_type":"stream"}]},{"cell_type":"code","source":"#By Man of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\ninputs = tokenizer.encode(en_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=128, num_return_sequences=4, num_beams=4)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T03:59:06.716023Z","iopub.execute_input":"2024-04-24T03:59:06.716928Z","iopub.status.idle":"2024-04-24T03:59:11.713390Z","shell.execute_reply.started":"2024-04-24T03:59:06.716892Z","shell.execute_reply":"2024-04-24T03:59:11.712390Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Instruments The ancient Egyptians use the transit for the first time about 4,000 years ago. The Egyptians were the first people to use the transit for the first time about 4,000 years ago. The Egyptians were the first people to use the transit for the first time about 4,000 years ago. The Egyptians were the first people to use the transit for the first time about 4,000 years ago. The Egyptians were the first people to use the transit for the first time about 4,000 years ago. The Egyptians were the first people to use the transit for the first time about 4,000 years ago. The Egyptians were the first people to use the transit for the first time about 4,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#The difference is on the Last paragraph:\n\nIt has one more line: \"If you or a loved one is struggling with a mental health condition, reach out to us today for a free consultation.\"","metadata":{}},{"cell_type":"code","source":"#By Man of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\nprint(tokenizer.decode(outputs[2], skip_special_tokens=True))#Original was outputs[1]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:25:53.294428Z","iopub.execute_input":"2023-12-12T18:25:53.295428Z","iopub.status.idle":"2023-12-12T18:25:53.302006Z","shell.execute_reply.started":"2023-12-12T18:25:53.295384Z","shell.execute_reply":"2023-12-12T18:25:53.301048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text2 = df.loc[21, \"text\"]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:29:35.233761Z","iopub.execute_input":"2023-12-12T18:29:35.234235Z","iopub.status.idle":"2023-12-12T18:29:35.239513Z","shell.execute_reply.started":"2023-12-12T18:29:35.2342Z","shell.execute_reply":"2023-12-12T18:29:35.238408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text2","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:29:56.648494Z","iopub.execute_input":"2023-12-12T18:29:56.648872Z","iopub.status.idle":"2023-12-12T18:29:56.655307Z","shell.execute_reply.started":"2023-12-12T18:29:56.648844Z","shell.execute_reply":"2023-12-12T18:29:56.654325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#By Man of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\ninputs = tokenizer.encode(text2, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=128, num_return_sequences=4, num_beams=4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:30:43.715965Z","iopub.execute_input":"2023-12-12T18:30:43.716842Z","iopub.status.idle":"2023-12-12T18:30:49.619864Z","shell.execute_reply.started":"2023-12-12T18:30:43.716809Z","shell.execute_reply":"2023-12-12T18:30:49.618907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:31:38.835751Z","iopub.execute_input":"2023-12-12T18:31:38.836671Z","iopub.status.idle":"2023-12-12T18:31:38.842798Z","shell.execute_reply.started":"2023-12-12T18:31:38.836635Z","shell.execute_reply":"2023-12-12T18:31:38.84174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Difference from above. That was added after \"the client and with his consent.\"\n\n#Check input 9. Compare with those lines. Where did the bigscience/bloom-1b7 take that???\n\n\" The therapist and the client work together to develop a treatment plan that is tailored to the needs of the client and his family.\n<HUMAN>: What is the role of the therapist in the treatment of addictions?\n<ASSISTANT>: The role of the therapist in the treatment of addictions is twofold. First, the therapist assists the client and his family in developing a treatment plan that is tailored to the needs of the client and his family. Second, the therapist works closely with the client and his family to ensure that the client and his family receive the support they need to overcome the challenges.\"","metadata":{}},{"cell_type":"code","source":"#By Man of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\nprint(tokenizer.decode(outputs[3], skip_special_tokens=True))#Original was outputs[1]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:33:40.635323Z","iopub.execute_input":"2023-12-12T18:33:40.635731Z","iopub.status.idle":"2023-12-12T18:33:40.642605Z","shell.execute_reply.started":"2023-12-12T18:33:40.635703Z","shell.execute_reply":"2023-12-12T18:33:40.641618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Acknowledgements:\n\nMan of the year https://www.kaggle.com/code/manwithaflower/bloom-1b7-gsm8k\n\nmpwolke  https://www.kaggle.com/code/mpwolke/socraticgsm8k-with-bloom-1b7","metadata":{}}]}